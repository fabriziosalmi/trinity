# Trinity Configuration
# Rule: Never commit the actual .env file
# Copy this to .env and configure with your actual values

# LM Studio Configuration (Local LLM)
LM_STUDIO_URL=http://localhost:1234/v1
# Or use a different local IP if LM Studio is on another machine:
# LM_STUDIO_URL=http://192.168.1.100:1234/v1

# OpenAI Configuration (Cloud LLM - Optional)
# OPENAI_API_KEY=sk-your-api-key-here

# Alternative LLM Provider (Ollama)
LLM_PROVIDER=ollama
LLM_MODEL_NAME=llama3.2:3b
LLM_BASE_URL=http://localhost:11434
LLM_TIMEOUT=30
LLM_MAX_RETRIES=3
LLM_TEMPERATURE=0.2

# Build Settings
DEFAULT_THEME=enterprise
VALIDATE_HTML=true
MINIFY_OUTPUT=false
DEBUG=false

# Paths (relative to project root)
TEMPLATE_DIR=library
OUTPUT_DIR=output
LOG_DIR=logs

# üß† Neural-Symbolic Architecture - Complete Pipeline

**Status:** ‚ö†Ô∏è Core Complete, Training Dataset Pending  
**Version:** v0.5.0 (Neural Healer with LSTM)  
**Date:** November 26, 2025

> **Reality Check:** This document describes the full vision of Trinity's neural-symbolic pipeline.  
> **What's Complete:** Predictor (RF), Neural Healer (LSTM), Centuria Factory infrastructure  
> **What's Pending:** The "100+ themes" training dataset has not been generated (current: 15 themes, 182 samples)  
> The transfer learning benefits described below are **aspirational** until the full dataset is created.

---

## üéØ The Vision: From Heuristics to Machine Learning

### Traditional Approach (v0.2.0)
```
Guardian detects overflow ‚Üí Apply CSS_BREAK_WORD ‚Üí Still broken? ‚Üí Try FONT_SHRINK ‚Üí etc.
```
**Problem:** Hard-coded strategies, no learning, slow iteration.

### Neural-Symbolic Approach (v0.3.0)
```
Model predicts breakage BEFORE rendering ‚Üí Apply optimal fix on first try ‚Üí Success
```
**Advantage:** Data-driven, learns from experience, 100x faster predictions.

### Centuria Factory Enhancement (v0.4.0)
```
100 diverse themes ‚Üí Data augmentation ‚Üí Model learns DOM physics, not theme patterns
```
**Breakthrough:** Training on 100+ themes forces the model to understand **universal layout rules** instead of memorizing theme-specific quirks.

---

## üèóÔ∏è Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              TRINITY CORE v0.4.0 (CENTURIA FACTORY)             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  SYMBOLIC    ‚îÇ    ‚îÇ    NEURAL    ‚îÇ    ‚îÇ    HYBRID    ‚îÇ
‚îÇ              ‚îÇ    ‚îÇ              ‚îÇ    ‚îÇ              ‚îÇ
‚îÇ  Jinja2 +    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  ML Models   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Predictive  ‚îÇ
‚îÇ  Tailwind    ‚îÇ    ‚îÇ  (Trained)   ‚îÇ    ‚îÇ  Self-Heal   ‚îÇ
‚îÇ              ‚îÇ    ‚îÇ              ‚îÇ    ‚îÇ              ‚îÇ
‚îÇ Deterministic‚îÇ    ‚îÇ  Adaptive    ‚îÇ    ‚îÇ  Intelligent ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                   ‚ñ≤                   ‚îÇ
        ‚îÇ                   ‚îÇ                   ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ DATA MINER ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
                      ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ  CENTURIA    ‚îÇ
              ‚îÇ  FACTORY     ‚îÇ
              ‚îÇ (100 Themes) ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
                      ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ   TRAINING   ‚îÇ
              ‚îÇ   DATASET    ‚îÇ
              ‚îÇ (5000+ rows) ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Key Components:**

1. **Symbolic Layer:** Deterministic templates (no hallucinations)
2. **Neural Layer:** ML models trained on real build data
3. **Data Miner:** Collects (content + style = outcome) tuples
4. **Hybrid Intelligence:** Combines symbolic structure with neural predictions

---

## üìä Phase 1: Data Collection (IMPLEMENTED)

### What Was Built

#### 1. **TrinityMiner** (`src/trinity/components/dataminer.py`)

**Purpose:** Log every build event with ML-ready features.

**Features Extracted:**
- `timestamp`: ISO-8601 build time
- `theme`: Theme name (e.g., 'brutalist')
- `input_char_len`: Total character count
- `input_word_count`: Total word count
- `css_signature`: MD5 hash of CSS overrides
- `active_strategy`: Healing strategy applied ('NONE', 'CSS_BREAK_WORD', etc.)
- `is_valid`: Guardian verdict (0 = broken, 1 = success)
- `failure_reason`: Error message from Guardian

**Key Methods:**
```python
from trinity.components.dataminer import TrinityMiner

miner = TrinityMiner()

# Log build event
miner.log_build_event(
    theme="brutalist",
    content=content_dict,
    strategy="CSS_BREAK_WORD",
    guardian_verdict=True,
    css_overrides={"hero_title": "break-all"}
)

# Get statistics
stats = miner.get_dataset_stats()
print(stats)
# {
#   "total_samples": 1000,
#   "positive_samples": 650,
#   "negative_samples": 350,
#   "success_rate": 65.0,
#   "themes": ["enterprise", "brutalist", "editorial"],
#   "strategies": ["NONE", "css_break_word", "font_shrink", ...]
# }
```

#### 2. **Engine Integration** (`src/trinity/engine.py`)

**Auto-Logging:** Every build is now logged automatically.

```python
# Positive Sample (Guardian approves)
if report["approved"]:
    self.miner.log_build_event(
        theme=theme,
        content=current_content,
        strategy=current_strategy,
        guardian_verdict=True,  # ‚úÖ
        guardian_reason="",
        css_overrides=current_style_overrides
    )

# Negative Sample (Guardian rejects)
else:
    self.miner.log_build_event(
        theme=theme,
        content=current_content,
        strategy=current_strategy,
        guardian_verdict=False,  # ‚ùå
        guardian_reason=report["reason"],
        css_overrides=current_style_overrides
    )
```

**Why Log Both?**
- Positive samples teach the model what "stable" looks like
- Negative samples teach what "broken" looks like
- Balanced dataset prevents bias

#### 3. **CLI Commands** (`src/trinity/cli.py`)

**trinity mine-stats**: View dataset statistics
```bash
$ trinity mine-stats

üìä Trinity ML Dataset Statistics

       Dataset Overview       
‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÉ Metric             ‚îÉ Value ‚îÉ
‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©
‚îÇ Total Samples      ‚îÇ 15    ‚îÇ
‚îÇ Positive (Success) ‚îÇ 0     ‚îÇ
‚îÇ Negative (Failure) ‚îÇ 15    ‚îÇ
‚îÇ Success Rate       ‚îÇ 0.0%  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Themes: editorial, enterprise
Strategies: NONE, css_break_word, font_shrink
```

**trinity mine-generate**: Generate synthetic training data
```bash
# Generate 1000 samples (overnight data mining)
$ trinity mine-generate --count 1000

# Generate brutalist layouts only
$ trinity mine-generate --count 500 --themes brutalist

# Fast mining (no Guardian - testing only)
$ trinity mine-generate --count 100 --no-guardian
```

**How It Works:**
1. Generate random content (normal text + pathological patterns)
2. Build with random theme
3. Guardian audits layout
4. If rejected, apply healing strategies
5. Log EVERY attempt (negative + positive samples)
6. Repeat `--count` times

**Pathological Patterns (20% probability):**
- `AAAAAAAAAA...` (unbreakable strings)
- `https://example.com/xxxxxxxxxx...` (long URLs)
- `NoSpacesJustOneVeryLongWordRepeated...` (CamelCase overflow)

**Why Pathological?** Forces Guardian to reject, creating valuable negative samples.

---

## üìÅ Dataset Structure

**File:** `data/training_dataset.csv`

**Headers:**
```csv
timestamp,theme,input_char_len,input_word_count,css_signature,active_strategy,is_valid,failure_reason
```

**Example Rows:**
```csv
2025-11-26T12:36:17.611540,enterprise,369,33,NONE,NONE,0,DOM overflow detected
2025-11-26T12:36:18.436232,enterprise,369,33,b0085dd53682,css_break_word,0,DOM overflow detected
2025-11-26T12:36:19.245361,enterprise,369,33,63769e362f28,font_shrink,0,DOM overflow detected
```

**Feature Interpretation:**
- `input_char_len=369`: Content has 369 total characters
- `css_signature=b0085dd53682`: Unique hash of CSS overrides
- `active_strategy=css_break_word`: Healer applied break-word strategy
- `is_valid=0`: Guardian rejected (negative sample)
- `failure_reason=DOM overflow detected`: Why it failed

---

## üöÄ Usage Workflow

### Step 1: Collect Data (Run Overnight)

```bash
# SSH into server or run locally
$ trinity mine-generate --count 5000 --guardian

# Let it run for hours/overnight
# Each build = 3-4 attempts = 3-4 samples
# 5000 builds √ó 3 attempts = 15,000+ samples
```

### Step 2: Check Dataset Quality

```bash
$ trinity mine-stats

# Look for:
# - Total samples > 1000 (minimum for training)
# - Success rate 30-70% (balanced dataset)
# - Multiple themes represented
# - All strategies present (NONE, css_break_word, font_shrink, etc.)
```

### Step 3: Export for Training (Future Phase 2)

```bash
# TODO: Implement in Phase 2
$ python scripts/train_predictor.py train --model risk-assessor
```

---

## üéì Machine Learning Models (Future Phases)

### Model A: Risk Assessor (Phase 2)
**Task:** Binary classification - predict layout breakage BEFORE rendering

**Input Features:**
- `input_char_len` (numeric)
- `input_word_count` (numeric)
- `theme` (one-hot encoded)
- `css_signature` (embedding)

**Output:**
- Probability of breakage: `[0.0 - 1.0]`

**Architecture Options:**
- Random Forest (interpretable, good for tabular data)
- XGBoost (high accuracy)
- LSTM (if treating CSS as sequence)

**Usage (Future):**
```python
from trinity.models import RiskAssessor

model = RiskAssessor.load("models/risk_assessor.pkl")

# Predict BEFORE building
prob_broken = model.predict(char_len=500, theme="brutalist", css={})

if prob_broken > 0.8:
    # Apply CSS fix preemptively!
    css_overrides = {"hero_title": "break-all"}
```

**Advantage:** Skip 1-2 seconds of Chrome rendering if we KNOW it will break.

### Model B: Style Generator (Phase 3)
**Task:** Generative model - create infinite CSS themes from vibe vector

**Input:**
- Vibe vector: `[chaos, professional, colorful, minimal]`
- Example: `[0.9, 0.1, 0.8, 0.2]` = "90% chaos, 10% professional, 80% colorful"

**Output:**
- Sequence of Tailwind classes: `bg-yellow-400 text-black border-4 border-black shadow-lg`

**Architecture:**
- LSTM with attention (trained on CSS sequences)
- VAE (Variational Autoencoder) for latent space interpolation

**Usage (Future):**
```python
from trinity.models import StyleGenerator

model = StyleGenerator.load("models/style_generator.pth")

# Generate theme from vibe
vibe = [0.5, 0.5, 0.3, 0.7]  # 50% chaos, 50% professional, ...
css_classes = model.generate(vibe)

print(css_classes)
# {
#   "hero_title": "text-4xl font-black bg-gradient-to-r from-purple-500",
#   "card_bg": "bg-white border-2 border-gray-300 shadow-md"
# }
```

**Advantage:** Never run out of themes. Interpolate between brutalist and minimal. Discover novel combinations.

---

## üìà Performance Predictions

### Current (v0.2.0): Heuristic Self-Healing
- Guardian + Chrome rendering: **1-2 seconds per build**
- 3 retry attempts: **3-6 seconds total**
- Success rate: **95%** (normal content), **100%** (with truncation)

### Future (v0.3.0+): ML-Predicted Self-Healing
- Model inference: **0.001 seconds** (1ms)
- Preemptive fix: **0 retries needed**
- Total build time: **1-2 seconds** (no retries)

**Speedup:** ~3x faster for pathological content (skip retry loop)

---

## ‚úÖ Testing & Validation

### Phase 1 Tests (Completed)
```bash
# Generate 5 samples with Guardian
$ trinity mine-generate --count 5 --guardian

‚úÖ Mining complete!
   Successful: 0
   Failed: 5
   Total samples: 5

Dataset now contains 15 total samples
```

**Result:** 15 samples collected (3 attempts each)

**CSV Validation:**
```bash
$ docker-compose exec trinity-builder head -5 data/training_dataset.csv

timestamp,theme,input_char_len,input_word_count,css_signature,active_strategy,is_valid,failure_reason
2025-11-26T12:36:17.611540,enterprise,369,33,NONE,NONE,0,DOM overflow detected
2025-11-26T12:36:18.436232,enterprise,369,33,b0085dd53682,css_break_word,0,DOM overflow detected
```

**‚úÖ All features present and correct.**

---

## üó∫Ô∏è Roadmap

### ‚úÖ Phase 1: Data Collection (DONE)
- [x] TrinityMiner implementation
- [x] Engine integration (auto-logging)
- [x] CLI commands (mine-stats, mine-generate)
- [x] Dataset structure validated
- [x] Initial 15 samples collected

### üöß Phase 2: Risk Assessor Training (Next)
- [ ] Collect 1000+ samples (run `trinity mine-generate --count 1000`)
- [ ] Implement feature extraction (one-hot encoding, normalization)
- [ ] Train Random Forest classifier
- [ ] Evaluate model (accuracy, precision, recall)
- [ ] Save model to `models/risk_assessor.pkl`
- [ ] Integrate into builder (preemptive fixes)

### üîÆ Phase 3: Style Generator Training (Future)
- [ ] Scrape CSS from top 1000 Tailwind sites
- [ ] Tokenize CSS class sequences
- [ ] Train LSTM on (vibe ‚Üí CSS) pairs
- [ ] Implement latent space interpolation
- [ ] Deploy generative theme API

### üåü Phase 4: Production Deployment
- [ ] A/B test ML vs heuristic strategies
- [ ] Monitor model drift (retrain periodically)
- [ ] Optimize inference speed (ONNX, quantization)
- [ ] Multi-model ensemble (Risk + Style + Guardian)

---

## üè≠ Phase 4: The Centuria Factory (IMPLEMENTED - v0.4.0)

### The Problem: Memorization vs Generalization

**Symptom:** Model trained on 3 themes (Enterprise, Brutalist, Editorial) achieves 0.918 F1-score but learns **theme-specific patterns**:

```python
# What the model "learns" with 3 themes:
if theme == "brutalist" and title_length > 100:
    return HIGH_RISK  # Memorized pattern
elif theme == "enterprise":
    return LOW_RISK   # "Enterprise is always safe"
```

**Problem:** This is **overfitting to theme names**, not understanding layout physics.

### The Solution: Data Augmentation for Frontend

Generate 100 diverse themes with:
- Font sizes: `text-xs` to `text-9xl`
- Padding: `p-0` to `p-24`
- Borders: none, thin, thick, glitch effects
- Colors: pastels, neons, monochromes, gradients

Force the model to learn **universal DOM rules**:

```python
# What the model learns with 100 themes:
if char_length > (container_width / avg_char_width):
    return HIGH_RISK  # Geometric constraint
elif font_size > container_height * 0.8:
    return HIGH_RISK  # Physical overflow
```

### Implementation

#### 1. Text-to-Theme Engine

**File:** `src/trinity/components/brain.py`

```python
def generate_theme_from_vibe(self, vibe_description: str) -> Dict[str, str]:
    """
    Convert natural language style description to TailwindCSS config.
    
    Examples:
        "Cyberpunk neon city" ‚Üí bg-black text-cyan-400 ...
        "Victorian steampunk" ‚Üí bg-amber-900 text-yellow-200 ...
    """
```

**CLI Command:**

```bash
trinity theme-gen "Vaporwave aesthetic pink teal glitch" --name vaporwave
```

#### 2. Mass Theme Generator

**File:** `scripts/mass_theme_generator.py`

**Categories (20 themes each):**

```python
THEME_CATEGORIES = {
    "historical": [  # Victorian, Art Deco, 1980s, Y2K...
        "Victorian steampunk with brass and dark wood",
        "Art Deco 1920s with gold geometric patterns",
        # ... 18 more
    ],
    "tech": [  # DOS Terminal, Material Design, Vaporwave...
        "DOS terminal green on black monospace",
        "Material Design bold colors and shadows",
        # ... 18 more
    ],
    "artistic": [  # Bauhaus, Cubist, Pop Art, Swiss...
        "Bauhaus geometric primary colors",
        "Pop Art bold comic book style",
        # ... 18 more
    ],
    "chaotic": [  # Glitch, Trashcore, Comic Sans...
        "Glitch art digital corruption artifacts",
        "Comic Sans ransom note aesthetic",
        # ... 18 more
    ],
    "professional": [  # Legal, Medical, Fintech, Gov...
        "Legal firm conservative navy blue",
        "Medical clean white sterile minimal",
        # ... 18 more
    ],
}
```

**Usage:**

```bash
# Generate all 100 themes
python scripts/mass_theme_generator.py

# Preview without generating
python scripts/mass_theme_generator.py --dry-run

# Generate specific category
python scripts/mass_theme_generator.py --category tech --count 20
```

#### 3. Training Pipeline with Diverse Themes

```bash
# Step 1: Generate 100 themes (takes ~15 minutes)
python scripts/mass_theme_generator.py

# Step 2: Collect 5000+ diverse training samples
trinity mine-generate --count 5000

# Step 3: Verify dataset diversity
trinity mine-stats
# Expected output:
# Total: 5000 samples
# Themes: 103 (enterprise, brutalist, editorial + 100 generated)
# Success rate: 40-60% (balanced dataset)

# Step 4: Train model on diverse data
trinity train

# Expected improvement:
# F1-Score: 0.918 ‚Üí 0.95+ (better generalization)
# Precision: 0.867 ‚Üí 0.90+ (fewer false positives)
# Recall: 0.975 ‚Üí 0.98+ (catches more edge cases)
```

### Expected Impact

| Metric | 3 Themes | 100 Themes | Improvement |
|--------|----------|------------|-------------|
| **F1-Score** | 0.918 | **0.95+** | +3.5% |
| **Precision** | 0.867 | **0.90+** | +4% |
| **Generalization** | ‚ùå Memorizes | ‚úÖ Understands | 100% |
| **Edge Cases** | 75% caught | **95% caught** | +20% |

### Why This Works (Mathematical Intuition)

**3 Themes:** Model has 3 degrees of freedom ‚Üí learns theme-specific rules.

**100 Themes:** Model has 100+ degrees of freedom ‚Üí **forced to find universal patterns** (DOM geometry, text overflow physics).

**Analogy:** Training a vision model on:
- 3 cat photos ‚Üí learns "this specific cat = cat"
- 1000 cat photos ‚Üí learns "four legs + whiskers + fur = cat"

---

## üõ†Ô∏è Development Guide

### Adding New Features to Dataset

Edit `src/trinity/components/dataminer.py`:

```python
def log_build_event(self, ...):
    # Add new feature
    font_family = self._extract_font_family(content)
    
    # Update CSV row
    writer.writerow([
        ...,
        font_family  # New column
    ])
```

Update CSV headers in `_ensure_dataset_exists()`.

### Custom Mining Strategies

Create script `scripts/custom_mine.py`:

```python
from trinity import TrinityEngine, TrinityConfig

config = TrinityConfig()
engine = TrinityEngine(config)

# Mine specific pattern
for i in range(1000):
    content = create_long_title_content()  # Custom generator
    engine.build_with_self_healing(content, "brutalist", f"mine_{i}.html", True)
```

---

## üìö References

**Papers:**
- "Neural-Symbolic Learning and Reasoning" (Garcez et al., 2012)
- "Combining Symbolic and Neural Approaches for Natural Language Understanding" (Andreas et al., 2016)

**Inspiration:**
- AlphaGo: Symbolic game rules + Neural value network
- GPT-4: Symbolic prompts + Neural transformer
- Trinity: Symbolic templates + Neural layout predictor

---

**Made with üß† by developers who believe in data-driven engineering.**

*"From heuristics to predictions. From rules to learning."* - Trinity Neural Philosophy
